{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a review and an exercise of USF msan694 course - distributed computing (Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.54:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Week 2:\n",
    "\n",
    "### Creation - two ways\n",
    "    1(a) loading an external data\n",
    "    1(b) taking a collection such as seq (array or list)\n",
    "\n",
    "### Tranformation: creating a new RDD, like map, filter, distinct.\n",
    "\n",
    "    2(a) Difference between map and flatMap: flatMap returns the one level structure\n",
    "\n",
    "### Action: trigger a computation to return the result of calling program or to perform some actions on RDD objects, like collect(), reduce(), fold(), aggregate(). Action returns non-RDDs.\n",
    "    \n",
    "    3(a)Compared to reduce, fold will take an initial values.\n",
    "    \n",
    "    3(b)Compared to reduce and fold which return same data type, aggregate take initial value(s) and is also able to return a different data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercises\n",
    "- RDD creation - method 1\n",
    "- Transformation - map/flatMap, filter, distinct\n",
    "- Function - split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"Documents/Spark/ignatian_pedagogy.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exercise: split line on space - map returns a list for each paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = lines.map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words2 = lines.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exercise: filter - map returns all paragraph containing \"USF\", flatmap returns the word \"USF\" only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = words2.filter(lambda lines: \"USF\" in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"USF's\", u'USF', u\"USF's\", u\"USF's\", u'USF', u'USF', u'USF', u'USF']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "exercise: distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distinct_words = words2.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'1981,', u'all', u'just', u'Father', u'actions']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_words.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: return count & sum for each RDD partition\n",
    "- RDD creation - Method 2 & specifying # of partitions\n",
    "- Transformation - mapPartitions\n",
    "- Action - reduce, aggregate\n",
    "- Function - create a function to pass in transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12], [13, 14, 15, 16]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums1 = sc.parallelize(range(1,17), 4)\n",
    "nums1.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums1.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def count_sum(nums):\n",
    "    count_sum = [0,0]\n",
    "    for num in nums:\n",
    "        count_sum[0] += 1\n",
    "        count_sum[1] += num\n",
    "    return [count_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[4, 10], [4, 26], [4, 42], [4, 58]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partition_count_sum = nums1.mapPartitions(count_sum)\n",
    "partition_count_sum.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further calculating average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_count_sum = partition_count_sum.reduce(lambda x,y: [x[0]+y[0], x[1]+y[1]])\n",
    "average = float(total_count_sum[1]/total_count_sum[0])\n",
    "average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise : a different way to calculate average\n",
    "- Action - reduce\n",
    "\n",
    "Note: action will break the partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 1], [1, 2], [1, 3], [1, 4]],\n",
       " [[1, 5], [1, 6], [1, 7], [1, 8]],\n",
       " [[1, 9], [1, 10], [1, 11], [1, 12]],\n",
       " [[1, 13], [1, 14], [1, 15], [1, 16]]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_num = nums1.map(lambda x: [1,x])\n",
    "one_num.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_sum = one_num.reduce(lambda x,y: [x[0]+y[0], x[1]+y[1]])\n",
    "average = float(count_sum[1]/count_sum[0]) # without float, it will return 8\n",
    "average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating count and sum using action - aggregate, even simplier than reduce\n",
    "- aggregate(zero)(SeqOp, combOp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 25)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_nums.aggregate((0,0),lambda x,y:(x[0]+1,x[1]+y), lambda x,y:(x[0]+y[0],x[1]+y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Difference between reduce and fold and aggregate: \n",
    "\n",
    "fold gives an initial value to each partition. Also when the RDD is empty, reduce will return error but fold will return zero given that the initial value is zero.\n",
    "\n",
    "Aggregate can return an element of different type.\n",
    "\n",
    "##### Similarity\n",
    "reduce and fold will only take two arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9])\n",
    "odd_nums = nums.filter(lambda x: x%2 ==1)   # get the odd numbers\n",
    "sum_odd = odd_nums.reduce(lambda x,y: x+y)\n",
    "sum_odd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odd_nums.fold(0, lambda x,y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nums2 = sc.parallelize([])\n",
    "nums2.fold(0,lambda x,y:x+y) # fold will give us zero, but reduce will give us an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. action: count().\n",
    "counts # of partition if there are more than one, or count # of elements in one partition.\n",
    "2. transformation: union(otherDataset), intersection(otherDataset)\n",
    "3. action: countByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = sc.parallelize([\"MSAN694\", \"MSAN694 Distributed Computing\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if split with map [['MSAN694'], ['MSAN694', 'Distributed', 'Computing']]\n",
    "if split with flatmap ['MSAN694', 'MSAN694', 'Distributed', 'Computing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.map(lambda x: x.split()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.flatMap(lambda x: x.split()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[3], [4], [1], [2], [2], [3], [4], [5]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# union will not merge partition, but gather all partitions.\n",
    "# intersection will keep elements that show up in both\n",
    "x = sc.parallelize([3,4,1,2],4)\n",
    "y = sc.parallelize(range(2,6),4)\n",
    "z = x.union(y)\n",
    "z.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {2: 1, 3: 1, 4: 1})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.countByValue()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RDD actions explaination\n",
    "    1. takeSample(withReplacement,num, seed)\n",
    "       sample(withReplacement, fraction or probability, seed) - transformation\n",
    "    2. top(n) return top n elements with order\n",
    "    3. take(n) take first n elements\n",
    "    4. first() return the first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 3, 4, 4, 3, 5]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.takeSample(False,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 2, 2, 3, 3, 1, 3, 2, 4, 2, 2, 3, 3, 1, 3, 3, 3, 4, 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.takeSample(True,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 4, 5, 5, 1, 3, 4, 4, 1, 1, 2, 4, 2, 2, 3, 5, 4, 1, 2]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.takeSample(True,20,1) # order is fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 2, 3]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample returns an RRD. return arbitrary number of elements\n",
    "z.sample(False, 0.5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 4, 3, 3, 2]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.top(6) # list of top 6 elements with order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 4]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.take(2) # take list of first 6 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.first() # return the first element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### numeric RRD action types. The partition does not matter\n",
    "\n",
    "count(), mean(), sum(), max(), min(), variance(), stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1180339887498949"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.stdev()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## week 3\n",
    "\n",
    "### Pair RDDs creation\n",
    "    must use map to create the tuple. If using flatMap, the structure is flat, then there is no tuple/pair.\n",
    "\n",
    "### Pair RDD Operations - transformation\n",
    "    (a) sortByKey()\n",
    "    \n",
    "    (b) groupByKey()\n",
    "    \n",
    "    (c) mapValues(func); flatMapValues(func): Pass values in each pair through a map function without changing the keys\n",
    "    \n",
    "    (d) reduceByKey(func): run parallel reduce operation for each key\n",
    "    \n",
    "    (e) combineByKey(createCombiner, mergeValue, mergeCombiners)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KEY value pair example\n",
    "lines = sc.textFile(\"Documents/Spark/README.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = lines.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len_word = words.map(lambda x: (len(x),x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, u'#'),\n",
       " (1, u'a'),\n",
       " (1, u'a'),\n",
       " (1, u'a'),\n",
       " (1, u'a'),\n",
       " (1, u'a'),\n",
       " (1, u'a'),\n",
       " (1, u'N'),\n",
       " (1, u'a'),\n",
       " (1, u'A'),\n",
       " (1, u'a'),\n",
       " (2, u'is'),\n",
       " (2, u'It'),\n",
       " (2, u'in'),\n",
       " (2, u'R,')]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_word.sortByKey().take(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions that return resultiterable:\n",
    " groupByKey() and  cogroup(secondDataset)\n",
    "\n",
    "\n",
    "### To display content of resultiterable, for example in a RRD pair:\n",
    "    1. map the resultiterable to a list map(lambda x:(x[0], list(x[1]))), in this case only x[1] is resultiterable\n",
    "    2. flatMapValues(lambda x:x) to get rid of the structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: display (length, word) tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped_len_word = len_word.groupByKey()\n",
    "#grouped_len_word.collect() #returns (key, resultiterable)\n",
    "#grouped_len_word.map(lambda x:(x[0], list(x[1]))).collect() # visualize what's inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SIMILARLY, flatMapValues will break the structure of set of values, so each key will be matched to one value\n",
    "#grouped_len_word.flatMapValues(lambda x:x).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (1, 3), (1, 4), (2, 3), (2, 4), (2, 5)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sc.parallelize({(1, (2,3,4)), (2, (3,4,5) ) })\n",
    "a.flatMapValues(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### obtain (word, occourence) pair example - 2 methods\n",
    "    1. groupByKey() then mapValues\n",
    "    2. reduceByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# method 1\n",
    "word_1 = words.map(lambda x: (x, 1))\n",
    "word_occurence = word_1.groupByKey()\n",
    "output = word_occurence.mapValues(lambda x: sum(x))\n",
    "#output.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# method 2 - a simiplier way to get(word, occourence) pair using reduceByKey\n",
    "output = word_1.reduceByKey(lambda x,y: x+y)\n",
    "output.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# foldByKey returns same result\n",
    "output = word_1.foldByKey(0,lambda x,y: x+y)\n",
    "output.collect() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combineByKey\n",
    "    createCombiner - function to be used as the very first aggregation step for each key\n",
    "    mergeValue - what to do when combiner is given a new value\n",
    "    mergeCombiners - how to merge combiners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create(length of words, (frequency, a list of words))\n",
    "len_pair = len_word.combineByKey(lambda x: (1,x),  # (key=len, (1,u'word1'))\n",
    "                  lambda x,y: (x[0]+1,x[1]+\",\"+y),  \n",
    "                    # key does not change. x=(1,word). => (len, (2, u'word1, word2'))\n",
    "                  lambda x,y: (x[0]+y[0], x[1]+\",\"+y[1]))\n",
    "#len_pair.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## week 4 \n",
    "### Pair RDD Operations - Actions\n",
    "    (a) subtractByKey:\n",
    "    return RDD pairs whose keys are not in the other data set\n",
    "    (b) join, rightOuterJoin, leftOuterJoin(otherDataset) - by key\n",
    "    (c) cogroup: return (key, (resultiterable, resultiterable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_lines = sc.textFile(\"Documents/Spark/filtered_registered_business_sf.csv\")\n",
    "business_words = business_lines.map(lambda line: line.split(','))\n",
    "business = business_words.map(lambda x: (x[0], x[1])).distinct()\n",
    "# business.collect() (zipcode, business name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "supervisor_line = sc.textFile(\"Documents/Spark/supervisor_sf.csv\")\n",
    "supervisor = supervisor_line.map(lambda line: line.split(','))\n",
    "supervisor_pair = supervisor.map(lambda x: (x[0], x[1])).distinct()\n",
    "# supervisor_pair.collect() #(zipcode, supervisor_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_without_supervisor = business.subtractByKey(supervisor_pair).values().distinct()\n",
    "#business_without_supervisor.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "first_num_pairs =sc.parallelize({(2,3),(1,2),(1,3),(2,4),(3,6)})\n",
    "second_num_pairs = sc.parallelize({(1,3),(2,2)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 3), (3, 6), (1, 2), (2, 4)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.subtract(second_num_pairs).collect() # subtract common pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 6)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.subtractByKey(second_num_pairs).collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)), (1, (3, 3)), (2, (3, 2)), (2, (4, 2))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.join(second_num_pairs).collect() \n",
    "# by common keys. keys remained the same, values if the first data are added with that from 2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)), (1, (3, 3)), (2, (3, 2)), (2, (4, 2))]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.rightOuterJoin(second_num_pairs).collect() # the keys are from the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, (2, 3)), (1, (3, 3)), (2, (3, 2)), (2, (4, 2)), (3, (6, None))]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.leftOuterJoin(second_num_pairs).collect()# the keys are from the left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, ([2, 3], [3])), (2, ([3, 4], [2])), (3, ([6], []))]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_num_pairs.cogroup(second_num_pairs).mapValues(lambda x: (list(x[0]), list(x[1]))).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94117', ((u'Slattery Joseph', u'San Francisco'), u'1')),\n",
       " (u'94117', ((u'Duffy Colette A', u'San Francisco'), u'1')),\n",
       " (u'94117', ((u'Watts Harrey David', u'San Francisco'), u'1')),\n",
       " (u'94117', ((u'Bigelow Jamie Marie', u'San Francisco'), u'1')),\n",
       " (u'94117', ((u'Cole Hardware Inc', u'San Francisco'), u'1'))]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (zip, (business name, supervisor id)) sorted by supervisor id\n",
    "business.join(supervisor_pair).sortBy(lambda x:x[1][1]).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = business.cogroup(supervisor_pair).mapValues(lambda x:(list(x[0]), list(x[1])))\n",
    "# output.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nums = sc.parallelize([1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "example 5\n",
    "\n",
    "Additional Actions - countByKey(), lookup(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business_lines = sc.textFile(\"Documents/Spark/filtered_registered_business_sf.csv\")\n",
    "business_words = business_lines.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "business = business_words.map(lambda x: (x[0], (x[1], x[3]) )).distinct()\n",
    "#business.collect() # (zip, (name, city) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(business.lookup(\"\")) # look up empty key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'94110', (u'\"Rlm Partners', u'60 29th St Ste 537')),\n",
       " (u'95492', (u'Denbeste Transportation Inc', u'Windsor')),\n",
       " (u'94104', (u'Belal Korin', u'Daily+city')),\n",
       " (u'95112', (u'James Costa', u'San+jose')),\n",
       " (u'94111', (u'Rosewood Venture Associates Iv', u'San+francisco'))]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business.filter(lambda x: \"San Francisco\" not in x[1][1]).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning Spark - persist in memory/disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile(\"Documents/Spark/README.md\")\n",
    "lines_with_Spark = lines.filter(lambda x: \"Spark\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[213] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.persist(StorageLevel.MEMORY_ONLY_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StorageLevel(False, True, False, False, 2)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the object cached?\n",
    "#lines_with_Spark.is_cached\n",
    "\n",
    "#StorageLevel(useDisk, useMemory, useOffHeap, deserialized, replication= 1)\n",
    "lines_with_Spark.getStorageLevel() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[213] at RDD at PythonRDD.scala:48"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines_with_Spark.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'rich set of higher-level tools including Spark SQL for SQL and DataFrames,',\n",
       " u'and Spark Streaming for stream processing.',\n",
       " u'You can find the latest Spark documentation, including a programming']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# you can check spark ui storage when action occured\n",
    "lines_with_Spark.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Last week - Loading and Saving Data\n",
    "Example 1: load csv files from a directory with key=filename, value=content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'file:/Users/QIAN/Documents/Spark/Assigned2/input_1/input.txt', u'Are you using Apache Spark 2.0?\\nYes, I am using Apache Spark 2.0! .25')\n"
     ]
    }
   ],
   "source": [
    "files = sc.wholeTextFiles('Documents/Spark/Assigned2/input_1')\n",
    "for file in files.take(1):\n",
    "    print file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 2: save data\n",
    "- saveAsTextFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('Documents/Spark/supervisor_sf.csv', 5)\n",
    "filtered_lines = lines.filter(lambda line: \"94103,\" in line)\n",
    "filtered_lines.saveAsTextFile(\"supervisor_94103\") # a folder of 5 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 3: import JSON file. \n",
    "- json.loads() change json format to dictionary\n",
    "- json.dumpts() changes back to json format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'{\"ID\":\"id_1\",\"array\":[1,2,3],\"dict\": {\"key\": \"value1\"}}',\n",
       " u'{\"ID\":\"id_2\",\"array\":[2,4,6],\"dict\": {\"key\": \"value2\"}}',\n",
       " u'{\"ID\":\"id_3\",\"array\":[3,6,9],\"dict\": {\"key\": \"value3\", \"extra_key\": \"extra_value3\"}}']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "input = sc.textFile(\"Documents/Spark/example.json\")\n",
    "input.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{u'ID': u'id_1', u'array': [1, 2, 3], u'dict': {u'key': u'value1'}},\n",
       " {u'ID': u'id_3',\n",
       "  u'array': [3, 6, 9],\n",
       "  u'dict': {u'extra_key': u'extra_value3', u'key': u'value3'}}]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = input.map(lambda x: json.loads(x))\n",
    "json_with_3 = data.filter(lambda x: 3 in x['array']) # records with 3 in array\n",
    "json_with_3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['{\"array\": [1, 2, 3], \"dict\": {\"key\": \"value1\"}, \"ID\": \"id_1\"}',\n",
       " '{\"array\": [3, 6, 9], \"dict\": {\"key\": \"value3\", \"extra_key\": \"extra_value3\"}, \"ID\": \"id_3\"}']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_with_3.map(lambda x: json.dumps(x)).saveAsTextFile(\"json_with_3\")\n",
    "# save in a folder with 2 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example 4: import csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# If the \"fieldnames\" parameter is omitted, the values in the first row of the csvfile will be used as the fieldnames.\n",
    "# Use .next() to get the next item. (Iterator)\n",
    "def csvLoader(line):\n",
    "    input = StringIO.StringIO(line)\n",
    "    reader = csv.DictReader(input, fieldnames=[\"Zip\",\"Supervisor\"]) \n",
    "    return reader.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Supervisor': '8', 'Zip': '94102'},\n",
       " {'Supervisor': '6', 'Zip': '94102'},\n",
       " {'Supervisor': '3', 'Zip': '94102'},\n",
       " {'Supervisor': '5', 'Zip': '94102'},\n",
       " {'Supervisor': '8', 'Zip': '94103'}]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = sc.textFile(\"Documents/Spark/supervisor_sf.csv\")\n",
    "csv_data = input.map(csvLoader)\n",
    "csv_data.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf_supervisor = csv_data.filter(lambda x :  (x['Zip']).startswith('94'))\n",
    "#sf_supervisor.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# csvWriter : Save in a format of \"Supervisor, Zip\".\n",
    "# csv.DictWriter : https://docs.python.org/2/library/csv.html#csv.DictWriter\n",
    "# Values in the dictionary passed to .writerow() are written to \"output\".\n",
    "def csvWriter(rdd):\n",
    "    output = StringIO.StringIO() # start as an empty string buffer\n",
    "    writer = csv.DictWriter(output, fieldnames=[\"Supervisor\", \"Zip\"])\n",
    "    writer.writerow(rdd)\n",
    "    return output.getvalue().strip() # strip() : delete \\r and \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8,94102', '6,94102', '3,94102', '5,94102', '8,94103']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data.map(csvWriter).take(5)\n",
    "#csv_data.map(csvWriter).saveAsTextFile(\"CSV_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
