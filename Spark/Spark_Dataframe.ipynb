{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://haz15659153122.cloud.wal-mart.com:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import * # StructField, StringType, ....\n",
    "from pyspark.sql.functions import * # from_json, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON dictionaries\n",
    "filename = \"data_august.txt\"\n",
    "filename2 = \"ab_test_lifestyle.csv\" \n",
    "filename3 = \"itemIds_582734.csv\"\n",
    "\n",
    "# standard CSV\n",
    "filename4 = \"ab_primary.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read First Data\n",
    "### 1. Read from json into Dataframe\n",
    "### 2. Add and delete comlumns\n",
    "### 3. Change one column from string type to array type and explode on that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- attributes: struct (nullable = true)\n",
      " |    |-- item_id: string (nullable = true)\n",
      " |    |-- primary_image_url: string (nullable = true)\n",
      " |    |-- secondary_image_urls: string (nullable = true)\n",
      " |-- productId: string (nullable = true)\n",
      " |-- productType: string (nullable = true)\n",
      " |-- tagSource: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read json\n",
    "df = spark.read.json(filename)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add / delete columns\n",
    "df = df.withColumn(\"itemId\", df['attributes']['item_Id']).\\\n",
    "    withColumn(\"primary_image_url\", df['attributes']['primary_image_url']).\\\n",
    "    withColumn(\"secondary_image_url\", df['attributes']['secondary_image_urls']).\\\n",
    "    drop(\"attributes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582726  rows\n",
      "1916234  rows\n"
     ]
    }
   ],
   "source": [
    "# convert secondary_image_url to array and expand\n",
    "df = df.withColumn(\"secondary_image_url\", split(regexp_replace(df[\"secondary_image_url\"], \"[\\\\[\\\\] ]\", \"\"), \",\") )\n",
    "print(df.count(), \" rows\")\n",
    "df = df.withColumn(\"secondary_image_url\", explode(df[\"secondary_image_url\"]))\n",
    "df = df.withColumn(\"secondary_image_url\", regexp_replace(df[\"secondary_image_url\"],\"[\\\"]\", \"\" ))\n",
    "print(df.count(), \" rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display one example of secondary url\n",
    "# df.select(df.secondary_image_url).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Second Data\n",
    "### 1. Read from json into Dataframe\n",
    "### 2. Display how many Null values are there if any\n",
    "### 3. Filter data by joining with another dataframe of ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999419\n"
     ]
    }
   ],
   "source": [
    "# read data\n",
    "df2 = spark.read.json(filename2)\n",
    "df2 = df2.withColumn(\"assetUrl\", df2['assetBag']['assetUrl']).\\\n",
    "    withColumn(\"assetType\", df2['assetBag']['assetType']).\\\n",
    "    drop(\"assetBag\")\n",
    "print(df2.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Any none value?\n",
      "+------+---------+--------+---------+\n",
      "|itemId|productId|assetUrl|assetType|\n",
      "+------+---------+--------+---------+\n",
      "|     0|        0|       0|        0|\n",
      "+------+---------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Any none value?\")\n",
    "df2.select([count(when(isnan(c), c)).alias(c) for c in df2.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "582734"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemids = spark.read.text(filename3)\n",
    "df2 = df2.alias('a').join(itemids.alias('b'),col('a.itemId') == col('b.value'),'inner')\n",
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Third Standard CSV data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read csv data with header\n",
    "df3 = spark.read.option(\"header\",True).csv(filename4).\\\n",
    "    withColumnRenamed('assetType', 'assetType2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join all three dataframes together \n",
    "### 1. Filter by columns matching\n",
    "### 1. Save uniqu item ids to txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joined = df.join(df2, ['productId', 'itemId'], how='left').\\\n",
    "    join(df3, ['productId', 'itemId'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "final= df_joined.filter(\n",
    "    (df_joined['secondary_image_url'] == df_joined['assetUrl']) & \\\n",
    "    (df_joined['assetType'] == 'SECONDARY') & \\\n",
    "    (df_joined['primary_image_url'] == df_joined['url']) & \\\n",
    "    (df_joined['assetType2'] == 'PRIMARY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.select('itemId').toPandas().to_csv('unchanged_itmes.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 Spark - local",
   "language": "python",
   "name": "spark-3-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
